{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering\n",
    "\n",
    "This recipe shows how we will be accessing the datasets necessary for the rest of the book.\n",
    "\n",
    "We start by loading the necessary libraries and resetting the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from tensorflow.python.framework import ops\n",
    "# ops.reset_default_graph()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Iris Dataset (R. Fisher / Scikit-Learn)\n",
    "\n",
    "One of the most frequently used ML datasets is the iris flower dataset.  We will use the easy import tool, `datasets` from scikit-learn.  You can read more about it here: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n",
      "[5.1 3.5 1.4 0.2]\n",
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()  # irisデータクラスのインスタンスを作成  dict-like型\n",
    "print(len(iris.data))\n",
    "print(len(iris.target))\n",
    "print(iris.data[0])\n",
    "print(set(iris.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Birthrate Dataset (Hosted on Github)\n",
    "\n",
    "The 'Low Birthrate Dataset' is a dataset from a famous study by Hosmer and Lemeshow in 1989 called, \"Low Infant Birth Weight Risk Factor Study\".  It is a very commonly used academic dataset mostly for logistic regression.  We will host this dataset on the public Github here:\n",
    "https://github.com/nfmcclure/tensorflow_cookbook/raw/master/01_Introduction/07_Working_with_Data_Sources/birthweight_data/birthweight.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "birthdata_url = 'https://github.com/nfmcclure/tensorflow_cookbook/' \\\n",
    "                'raw/master/01_Introduction/07_Working_with_Data_Sources/' \\\n",
    "                'birthweight_data/birthweight.dat'\n",
    "birth_file = requests.get(birthdata_url)\n",
    "# birth_fileのtextを'\\r\\n'で分割 -> データの行を要素としたリスト(birth_data)ができる\n",
    "birth_data = birth_file.text.split('\\r\\n')  # \\r はCRを意味する．昔のMacでの改行文字コードだった\n",
    "# birth_data の0要素め(=特徴名)をタブで分割 -> 特徴名を要素としたリスト(=ヘッダ)ができる\n",
    "birth_header = birth_data[0].split('\\t')\n",
    "# 各要素に特徴のリストをもつリストを作成\n",
    "birth_data = [[float(x) for x in y.split('\\t') if len(x) >= 1]\n",
    "              for y in birth_data[1:] if len(y) >= 1]\n",
    "print(len(birth_data))\n",
    "print(len(birth_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[requestsモジュールnについて](https://qiita.com/sqrtxx/items/49beaa3795925e7de666)\n",
    "\n",
    "<br>\n",
    "上のCodeで\n",
    "```python\n",
    "birth_data = [[float(x) for x in y.split('\\t') if len(x) >= 1]\n",
    "              for y in birth_data[1:] if len(y) >= 1]\n",
    "```\n",
    "ではリスト内包表記のネストが使われている． 詳しくは以下を参照  \n",
    "https://qiita.com/y__sama/items/a2c458de97c4aa5a98e7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for文\n",
      "(6, 6) | (6, 7) | (6, 8) | (6, 9) | (7, 6) | (7, 7) | (7, 8) | (7, 9) | (8, 6) | (8, 7) | (8, 8) | (8, 9) | (9, 6) | (9, 7) | (9, 8) | (9, 9) | "
     ]
    }
   ],
   "source": [
    "print(\"for文\")\n",
    "for i in range(10):\n",
    "    if i > 5:\n",
    "        for j in range(10):\n",
    "            if j > 5:\n",
    "                print((i, j), end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネストのリスト内包表記\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(6, 0), (7, 0), (8, 0), (9, 0)],\n",
       " [(6, 1), (7, 1), (8, 1), (9, 1)],\n",
       " [(6, 2), (7, 2), (8, 2), (9, 2)],\n",
       " [(6, 3), (7, 3), (8, 3), (9, 3)],\n",
       " [(6, 4), (7, 4), (8, 4), (9, 4)],\n",
       " [(6, 5), (7, 5), (8, 5), (9, 5)],\n",
       " [(6, 6), (7, 6), (8, 6), (9, 6)],\n",
       " [(6, 7), (7, 7), (8, 7), (9, 7)],\n",
       " [(6, 8), (7, 8), (8, 8), (9, 8)],\n",
       " [(6, 9), (7, 9), (8, 9), (9, 9)]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ネストのリスト内包表記\")\n",
    "# 先にjのforが周り, jが決まったら　内側のリスト内包表記(i の for)が走る\n",
    "[[(i, j) for i in range(10) if i > 5] for j in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネストのリスト内包表記 外側にif文あり\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(6, 6), (7, 6), (8, 6), (9, 6)],\n",
       " [(6, 7), (7, 7), (8, 7), (9, 7)],\n",
       " [(6, 8), (7, 8), (8, 8), (9, 8)],\n",
       " [(6, 9), (7, 9), (8, 9), (9, 9)]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ネストのリスト内包表記 外側にif文あり\")\n",
    "[[(i, j) for i in range(10) if i > 5] for j in range(10) if j > 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing Price Dataset (UCI)\n",
    "\n",
    "We will also use a housing price dataset from the University of California at Irvine (UCI) Machine Learning Database Repository. It is a great regression dataset to use.  You can read more about it here:  \n",
    "https://archive.ics.uci.edu/ml/datasets/Housing\n",
    "\n",
    "housing data:  \n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size 506\n",
      "feature size 14\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "housing_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/' \\\n",
    "                'housing/housing.data'\n",
    "housing_header = [\n",
    "    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
    "    'PTRATIO', 'B', 'LSTAT', 'MEDV'\n",
    "]\n",
    "housing_file = requests.get(housing_url)\n",
    "housing_data = [[float(x) for x in y.split(' ') if len(x) >= 1]\n",
    "                for y in housing_file.text.split('\\n') if len(y) >= 1]\n",
    "print('data size', len(housing_data))\n",
    "print('feature size', len(housing_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Handwriting Dataset (Yann LeCun)\n",
    "\n",
    "The MNIST Handwritten digit picture dataset is the `Hello World` of image recognition. The famous scientist and researcher, Yann LeCun, hosts it on his webpage here, http://yann.lecun.com/exdb/mnist/ .  But because it is so commonly used, many libraries, including TensorFlow, host it internally.  We will use TensorFlow to access this data as follows.\n",
    "\n",
    "If you haven't downloaded this before, please wait a bit while it downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "55000\n",
      "10000\n",
      "5000\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# one hot 表現にしてデータを取得\n",
    "mnist = input_data.read_data_sets(\"/tmp/MNIST_data/\", one_hot=True)\n",
    "print(len(mnist.train.images))\n",
    "print(len(mnist.test.images))\n",
    "print(len(mnist.validation.images))\n",
    "print(mnist.train.labels[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Data\n",
    "\n",
    "The CIFAR-10 data ( https://www.cs.toronto.edu/~kriz/cifar.html ) contains 60,000 32x32 color images of 10 classes collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.  Alex Krizhevsky maintains the page referenced here.  This is such a common dataset, that there are built in functions in TensorFlow to access this data (the keras wrapper has these commands). Note that the keras wrapper for these functions automatically splits the images into a 50,000 training set and a 10,000 test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this command requires an internet connection and a few minutes to download all the images.\n",
    "# dawnladsすると~/.keras/datasets に保存され，２回目からは高速にリストアできる\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.keras.datasets.cifar10.load_data()`関数については以下参照  \n",
    "https://keras.io/ja/datasets/  \n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10/load_data\n",
    "\n",
    "<br>\n",
    "The ten categories are (in order):\n",
    "\n",
    "<ol start=\"0\">\n",
    "  <li>Airplane</li>\n",
    "  <li>Automobile</li>\n",
    "  <li>Bird</li>\n",
    "  <li>Car</li>\n",
    "  <li>Deer</li>\n",
    "  <li>Dog</li>\n",
    "  <li>Frog</li>\n",
    "  <li>Horse</li>\n",
    "  <li>Ship</li>\n",
    "  <li>Truck</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6], dtype=uint8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0,] # this is a frog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125323128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the 0-th image (a frog)\n",
    "%matplotlib inline\n",
    "img = Image.fromarray(X_train[0,:,:,:])\n",
    "plt.imshow(img)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ham/Spam Texts Dataset (UCI)\n",
    "\n",
    "We will use another UCI ML Repository dataset called the SMS Spam Collection.  You can read about it here: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection .  As a sidenote about common terms, when predicting if a data point represents 'spam' (or unwanted advertisement), the alternative is called 'ham' (or useful information).\n",
    "\n",
    "This is a great dataset for predicting a binary outcome (spam/ham) from a textual input.  This will be very useful for short text sequences for Natural Language Processing (Ch 7) and Recurrent Neural Networks (Ch 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_data[0]: ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "text_data[0]: ['ham', 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...']\n",
      "5574\n",
      "{'ham', 'spam'}\n",
      "Ok lar... Joking wif u oni...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Get/read zip file\n",
    "zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/' \\\n",
    "            '00228/smsspamcollection.zip'\n",
    "r = requests.get(zip_url)\n",
    "z = ZipFile(io.BytesIO(r.content))\n",
    "file = z.read('SMSSpamCollection')  # zipファイル内のSMSSpamCollectionファイルを読み込む\n",
    "# Format Data\n",
    "text_data = file.decode(encoding='utf-8')  # byte型を文字列型に変更. 'UTF-8'のバイト文字列と思い復号\n",
    "# UTF-8をasciiのバイト文字列に符号化.  errors='ignore'により，変換できなかった文字列は取り除かれる\n",
    "text_data = text_data.encode('ascii', errors='ignore')\n",
    "text_data = text_data.decode(encoding='utf-8').split(\n",
    "    '\\n')  # byte型を文字列型に変更. 'UTF-8'のバイト文字列と思い復号\n",
    "print('text_data[0]:', text_data[0])\n",
    "text_data = [x.split('\\t') for x in text_data if len(x) >= 1]\n",
    "print('text_data[0]:', text_data[0])\n",
    "[text_data_target, text_data_train] = [list(x) for x in zip(*text_data)]\n",
    "print(len(text_data_train))\n",
    "print(set(text_data_target))\n",
    "print(text_data_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  (1, 3)\n",
      "list(i): [1, 3]\n",
      "i:  (2, 4)\n",
      "list(i): [2, 4]\n",
      "\n",
      "i:  (1, 3)\n",
      "i:  (2, 4)\n"
     ]
    }
   ],
   "source": [
    "# リストは “*”(アスタリスク) 演算子を使ってアンパックすると各要素を関数の引数として渡すことができるらしい。\n",
    "for i in zip(*[[1,2], [3,4]]):  # zip(*[[1,2], [3,4]]) = zip([1,2], [3, 4]) \n",
    "    print('i: ', i)\n",
    "    print('list(i):', list(i))\n",
    "print()\n",
    "for i in zip([1,2], [3,4]):\n",
    "    print('i: ', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-' \\\n",
    "            'databases/00228/smsspamcollection.zip'\n",
    "r = requests.get(zip_url)\n",
    "z = ZipFile(io.BytesIO(r.content))\n",
    "file = z.read('SMSSpamCollection')\n",
    "```\n",
    "に置いて...\n",
    "<ol start=\"1\">\n",
    "<li>r.content は getした内容をバイナリ形式で得るメソッド</li>\n",
    "<li>io.BytesIOはbyte-likeオブジェクトを受け取り， それをバイナリ形式のファイルストリームに変換する． </li> \n",
    "<li>ファイルストリームとはファイルとプログラムとの間の通り道のこと</li>\n",
    "<li>ZipFileによってファイルストリームからZipFileオブジェクトを生成する</li>\n",
    "</ol>\n",
    "参照: https://ja.stackoverflow.com/questions/37573/zip-io-bytesio%E3%81%AE%E6%84%8F%E5%91%B3-%E4%BD%BF%E3%81%84%E6%96%B9%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6\n",
    "\n",
    "<br>\n",
    "ファイルストリームとはプログラムとファイルとの間のデータの通り道のこと  \n",
    "参照: http://tech.nikkeibp.co.jp/it/article/COLUMN/20100225/344994/\n",
    "\n",
    "<br>\n",
    "python unpackについて  \n",
    "http://momijiame.tumblr.com/post/33081357208/python-%E3%81%A7%E3%83%AA%E3%82%B9%E3%83%88%E3%82%92%E5%BC%95%E6%95%B0%E3%81%AB%E3%82%A2%E3%83%B3%E3%83%91%E3%83%83%E3%82%AF%E3%81%99%E3%82%8B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Review Data (Cornell)\n",
    "\n",
    "The Movie Review database, collected by Bo Pang and Lillian Lee (researchers at Cornell), serves as a great dataset to use for predicting a numerical number from textual inputs.\n",
    "\n",
    "You can read more about the dataset and papers using it here:\n",
    "https://www.cs.cornell.edu/people/pabo/movie-review-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5331\n",
      "5331\n",
      "simplistic , silly and tedious . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "import tarfile\n",
    "\n",
    "movie_data_url = 'http://www.cs.cornell.edu/people/pabo/' \\\n",
    "                    'movie-review-data/rt-polaritydata.tar.gz'\n",
    "r = requests.get(movie_data_url)\n",
    "# Stream data into temp object\n",
    "stream_data = io.BytesIO(r.content)\n",
    "tmp = io.BytesIO()\n",
    "while True:  # ここのwhile文はtmpにstream_dataを書き込んでいる?\n",
    "    s = stream_data.read(16384)  # 16384B読み込む\n",
    "    if not s:  \n",
    "        break\n",
    "    tmp.write(s)  # 読み込んだものがから出ないならば書き込む\n",
    "stream_data.close()\n",
    "tmp.seek(0)  # ストリーム位置を指定された offset(=0) バイトに変更\n",
    "# Extract tar file\n",
    "tar_file = tarfile.open(fileobj=tmp, mode=\"r:gz\")  # Tarfileオブジェクトを生成\n",
    "# アーカイブから'rt-polaritydata/rt-polarity.pos'をファイルオブジェクトとして抽出する\n",
    "pos = tar_file.extractfile('rt-polaritydata/rt-polarity.pos')\n",
    "neg = tar_file.extractfile('rt-polaritydata/rt-polarity.neg')\n",
    "# Save pos/neg reviews\n",
    "pos_data = []\n",
    "for line in pos:  # 一行づつ読み込む\n",
    "    pos_data.append(line.decode('ISO-8859-1').encode('ascii',errors='ignore').decode())\n",
    "neg_data = []\n",
    "for line in neg:\n",
    "    neg_data.append(line.decode('ISO-8859-1').encode('ascii',errors='ignore').decode())\n",
    "tar_file.close()\n",
    "\n",
    "print(len(pos_data))\n",
    "print(len(neg_data))\n",
    "print(neg_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- アーカイブ\n",
    "  \n",
    "  ファイルやディレクトリなどを単純に一箇所に集めたもののこと。圧縮してファイルサイズを小さくすることは含まれない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete Works of William Shakespeare (Gutenberg Project)\n",
    "\n",
    "For training a TensorFlow Model to create text, we will train it on the complete works of William Shakespeare.  This can be accessed through the good work of the Gutenberg Project.  The Gutenberg Project frees many non-copyright books by making them accessible for free from the hard work of volunteers.\n",
    "\n",
    "You can read more about the Shakespeare works here:\n",
    "http://www.gutenberg.org/ebooks/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5582212\n"
     ]
    }
   ],
   "source": [
    "# The Works of Shakespeare Data\n",
    "import requests\n",
    "\n",
    "shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "# Get Shakespeare text\n",
    "response = requests.get(shakespeare_url)\n",
    "shakespeare_file = response.content\n",
    "# Decode binary into string\n",
    "shakespeare_text = shakespeare_file.decode('utf-8')\n",
    "# Drop first few descriptive paragraphs.\n",
    "shakespeare_text = shakespeare_text[7675:]\n",
    "print(len(shakespeare_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From fairest creatures we desire increase,\r\n",
      "  That thereby beauty's rose might never die,\r\n",
      "  But as \n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[: 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English-German Sentence Translation Database (Manythings/Tatoeba)\n",
    "\n",
    "The Tatoeba Project is also run by volunteers and is set to make the most bilingual sentence translations available between many different languages.  `Manythings.org` compiles the data and makes it accessible.\n",
    "\n",
    "http://www.manythings.org/corpus/about.html#info\n",
    "\n",
    "More bilingual sentence pairs: http://www.manythings.org/bilingual/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159204\n",
      "159204\n",
      "['Hello!', 'Hallo!']\n"
     ]
    }
   ],
   "source": [
    "# English-German Sentence Translation Data\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "sentence_url = 'http://www.manythings.org/anki/deu-eng.zip'\n",
    "r = requests.get(sentence_url)\n",
    "z = ZipFile(io.BytesIO(r.content))\n",
    "file = z.read('deu.txt')\n",
    "# Format Data\n",
    "eng_ger_data = file.decode()\n",
    "eng_ger_data = eng_ger_data.encode('ascii',errors='ignore')\n",
    "eng_ger_data = eng_ger_data.decode().split('\\n')\n",
    "eng_ger_data = [x.split('\\t') for x in eng_ger_data if len(x)>=1]\n",
    "[english_sentence, german_sentence] = [list(x) for x in zip(*eng_ger_data)]\n",
    "print(len(english_sentence))\n",
    "print(len(german_sentence))\n",
    "print(eng_ger_data[10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
